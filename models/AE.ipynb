{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Flatten, Dense, Conv2D, LeakyReLU, Reshape, Conv2DTranspose, Activation, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder class. It builds an internal encoder, and an internal decoder\n",
    "class Autoencoder:\n",
    "    def __init__(self, \n",
    "                input_dim,\n",
    "                encoder_conv_filters,\n",
    "                encoder_conv_kernel_size,\n",
    "                encoder_conv_strides,\n",
    "                decoder_conv_t_filters,\n",
    "                decoder_conv_t_kernel_size,\n",
    "                decoder_conv_t_strides,\n",
    "                z_dim,\n",
    "                variational = False,\n",
    "                r_loss_factor = 1000):\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.encoder_conv_filters = encoder_conv_filters\n",
    "        self.encoder_conv_kernel_size = encoder_conv_kernel_size\n",
    "        self.encoder_conv_strides = encoder_conv_strides\n",
    "        self.decoder_conv_t_filters = decoder_conv_t_filters\n",
    "        self.decoder_conv_t_kernel_size = decoder_conv_t_kernel_size\n",
    "        self.decoder_conv_t_strides = decoder_conv_t_strides\n",
    "        self.z_dim = z_dim\n",
    "        self.variational = variational\n",
    "        self.r_loss_factor = r_loss_factor\n",
    "    \n",
    "        self.n_layers_encoder = len(encoder_conv_filters)\n",
    "        self.n_layers_decoder = len(decoder_conv_t_filters)\n",
    "        \n",
    "        assert len(encoder_conv_filters) == len(encoder_conv_kernel_size) == len(encoder_conv_strides), 'len of encoder input params must be the same'\n",
    "        assert len(decoder_conv_t_filters) == len(decoder_conv_t_kernel_size) == len(decoder_conv_t_strides), 'len of decoder input params must be the same'\n",
    "        \n",
    "        if variational == False:\n",
    "            assert r_loss_factor == 1, 'r_loss_factor is not 1, but the encoder is not variational. r_loss_factor is ignored if not a variational encoder'\n",
    "        \n",
    "        encoder, encoder_input, encoder_output, shape_before_flattening = self.__build_encoder()\n",
    "        decoder, decoder_input = self.__build_decoder(shape_before_flattening)\n",
    "        \n",
    "        model_input = encoder_input\n",
    "        model_output= decoder(encoder_output)\n",
    "        \n",
    "        self.model = Model(model_input, model_output)\n",
    "    \n",
    "        self.__compile(learning_rate = 0.0005)\n",
    "        \n",
    "    def fit(self, x_train, batch_size, shuffle, epochs, callbacks):\n",
    "        \n",
    "        self.model.fit(\n",
    "                      x = x_train,\n",
    "                      y = x_train,\n",
    "                      batch_size = batch_size,\n",
    "                      shuffle = shuffle,\n",
    "                      epochs = epochs,\n",
    "                      callbacks = callbacks\n",
    "                      )\n",
    "    \n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "    \n",
    "    def load_model(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    # Optional: test different loss functions\n",
    "    def __compile(self, learning_rate):\n",
    "        optimizer = Adam(lr = learning_rate)\n",
    "        \n",
    "        if self.variational == True:\n",
    "            self.model.compile(optimizer = optimizer, loss = self.__vae_loss, metrics = [self.__vae_r_loss, self.__vae_kl_loss])\n",
    "        else:\n",
    "            self.model.compile(optimizer = optimizer, loss = self.__vae_r_loss,)\n",
    "        \n",
    "    def __vae_r_loss(self, y_true, y_pred):\n",
    "        r_loss = K.mean(K.square(y_true - y_pred), axis=[1,2,3])\n",
    "        if self.variational == True:\n",
    "            return self.r_loss_factor * r_loss\n",
    "        else:\n",
    "            return r_loss\n",
    "    \n",
    "    def __vae_kl_loss(self, y_true, y_pred):\n",
    "        kl_loss = -0.5 * K.sum(1 + self.log_var - K.square(self.mu) - K.exp(self.log_var), axis = 1)\n",
    "        return kl_loss\n",
    "    \n",
    "    def __vae_loss(self, y_true, y_pred):\n",
    "        r_loss = self.__vae_r_loss(y_true, y_pred)\n",
    "        kl_loss = self.__vae_kl_loss(y_true, y_pred)\n",
    "        return r_loss + kl_loss\n",
    "    \n",
    "    def __build_encoder(self):\n",
    "        # Create the input layer\n",
    "        encoder_input = Input(shape = self.input_dim, name = 'encoder_input')\n",
    "        \n",
    "        x = encoder_input\n",
    "        \n",
    "        # Create the intermediate layers. Each intermediate layer is composed of a Conv2D layer and a\n",
    "        # LeakyReLU layer.\n",
    "        for i in range(self.n_layers_encoder):\n",
    "            conv_layer = Conv2D(filters = self.encoder_conv_filters[i],\n",
    "                                kernel_size = self.encoder_conv_kernel_size[i],\n",
    "                                strides = self.encoder_conv_strides[i],\n",
    "                                padding = 'same',\n",
    "                                name = 'encoder_conv_' + str(i)\n",
    "                               )\n",
    "            x = conv_layer(x)\n",
    "            x = LeakyReLU()(x)\n",
    "        \n",
    "        # K.int_shape returns the shape of the vector as a tuple of integers (or None)\n",
    "        # Skip the first element, as it's the size of the batch\n",
    "        shape_before_flattening = K.int_shape(x)[1:]\n",
    "        \n",
    "        # Flatten the layer before connecting it to a Dense (Activation) layer\n",
    "        x = Flatten()(x)\n",
    "        \n",
    "        # If it's a variational encoder, build the decoder so it encode the images to two vectors,\n",
    "        # the mu (mean point of distribution) and the log_var (lgo of variance of each dimension)\n",
    "        # This encoder will have two outputs\n",
    "        if self.variational == True:\n",
    "            self.mu = Dense(self.z_dim, name = 'mu')(x)\n",
    "            self.log_var = Dense(self.z_dim, name = 'log_var')(x)\n",
    "            \n",
    "            encoder_mu_log_var = Model(encoder_input, (self.mu, self.log_var))\n",
    "            \n",
    "            # Sampling function that receives mu and log_var and returns a point\n",
    "            # sampled from the normal distribution function defined by mu and log_var\n",
    "            def sampling(args):\n",
    "                mu, log_var = args\n",
    "                epsilon = K.random_normal(shape = K.shape(mu), mean = 0., stddev = 1.)\n",
    "                return mu + K.exp(log_var / 2) * epsilon\n",
    "            \n",
    "            # Create a layer from a lambda\n",
    "            encoder_output = Lambda(sampling, name = 'encoder_output')([self.mu, self.log_var])\n",
    "        \n",
    "        else:\n",
    "            encoder_output = Dense(self.z_dim, name = 'encoder_output')(x)\n",
    "        \n",
    "        return Model(encoder_input, encoder_output), encoder_input, encoder_output, shape_before_flattening\n",
    "    \n",
    "    \n",
    "    def __build_decoder(self, shape_before_flattening):\n",
    "        decoder_input = Input(shape = (self.z_dim,), name = 'decoder_input')\n",
    "        \n",
    "        # Connect input to a dense layer\n",
    "        x = Dense(np.prod(shape_before_flattening))(decoder_input)\n",
    "        x = Reshape(shape_before_flattening)(x)\n",
    "        \n",
    "        for i in range(self.n_layers_decoder):\n",
    "            conv_t_layer = Conv2DTranspose(\n",
    "                filters = self.decoder_conv_t_filters[i],\n",
    "                kernel_size = self.decoder_conv_t_kernel_size[i],\n",
    "                strides = self.decoder_conv_t_strides[i],\n",
    "                padding = 'same',\n",
    "                name = 'decoder_conv_t_' + str(i)\n",
    "            )\n",
    "        \n",
    "            x = conv_t_layer(x)\n",
    "            \n",
    "            if i < self.n_layers_decoder - 1:\n",
    "                x = LeakyReLU()(x)\n",
    "            else:\n",
    "                x = Activation('sigmoid')(x)\n",
    "            \n",
    "        decoder_output = x\n",
    "        \n",
    "        return Model(decoder_input, decoder_output), decoder_output\n",
    "        \n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generative",
   "language": "python",
   "name": "generative"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
